---
title: The link between the probability realm and error minimisation (I)
date: 2025-08-25 14:08:00 +0200
categories: [Metrics, Insights]
tags: [regularization, probability, log-likelihood, error, metrics]     # TAG names should always be lowercase
author: pabaldonedo
description: Insights on the relation of mean square error and log likelihood
math: true
---

Many students, including myself, start their journey in Data Science enrolling in courses like the famous [Machine Learning Course](https://www.deeplearning.ai/courses/machine-learning-specialization/) by Andrew Ng at Coursera. They promptly find themselves before an equation looking something like this:

$$
\begin{equation}
  J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \frac{1}{2} (h_{\theta}(x^{(i)}) - y^{(i)})^2
  \label{eq:mse_andrew}
\end{equation}
$$

This is the mean square error (MSE) loss and we are taught that we must find the parameters  $$\theta$$ that minimise it. However, I remember that when I started looking into papers I began to find a different equation like:

$$
\begin{equation}
  \text{Max Log Likelihood} = \max_{\theta} p(y | x, \theta)
  \label{eq:max_likelihood}
\end{equation}
$$


Where does that probability come from? I was training models minimising the MSE, no probabilities were involved! I felt completely confused.

In these series of post we will close the gap between those two expressions and give the insights I wish I had when I was a student.

In this first post we will see why probabilities make sense when we talk about training and modelling processes and the link between MSE and Max log-likelihood. In the second post we will dive into the link between L2 regularisation and bayesian probability.

## Probability and machine learning models

All machine learning projects start by collecting some data which may come from a natural phenomenon (e.g. temperatures, rainfall, etc.) or business problem (e.g. user churn, product quality, pricing, etc.). The end goal is to build a model (i.e. mathematical function) that from the data recreates the natural phenomenen or business problem generating it. This is called the inverse problem.

For example, users in an online marketplace generates data with their behaviour (clicks, past purchases, session time, etc.). The business case is to know if a user is interested in a new perfume we are selling. Interested users will generate data different to uninterested ones, e.g. they will likely spend more time in other perfume and personal care products. Now our goal is to go in the oposite direction: from the data generated by a user to his type (interested or uninstered). We do this by buildind a model that predicts interest inferring from the user data.

<img src="/assets/img/ml_project/data_generation_process_with_text.png" width="100%" />


Let's dive deep with another example: house pricing. The goal is to map house information (e.g. size, floor, location, etc.) with its price. Imagine this simple logarithmic relationship:

$$
\begin{equation}
  price = \log(size) \\
  \label{eq:simplest_pricing}
\end{equation}
$$

This means that the price is solely determined by the house size and, therefore, for a given size the price is fixed. However, in reality we expect that for several homes of the exact same size the price will not be exactly the same, so a more accurate expresion would be:


$$
\begin{equation}
  price = \log(size) + \delta + \epsilon
  \label{eq:simplest_pricing_with_errors}
\end{equation}
$$

Where $$\delta$$ accounts for the error coming from other factors we are leaving out (e.g. location) and $$\epsilon$$ is noise. This noise could come from just errors in data collection for example or for unmeasurable factors like seller mood that can slightly vary the price. Note that even if we considered all factors that determine the price (i.e. $$\delta = 0$$) we expect to have some error $$\epsilon$$. We will assume $$\delta =0$$ for clarity in the rest of the post.

Let's assume $$\epsilon \sim \mathcal{N}(0, \sigma)$$, i.e. it comes from a normal distribution of zero mean and $$\sigma$$ variance, which is a common scenario.

So we expect the reality to be something like in the image below, where we have a normal distribution of prices revolving around the curve that is defined by the line $$price = \omega  \cdot \log(size)$$.

<img src="/assets/img/probability/log_surface_price_rotated.svg" width="100%" />

Let's pause and ponder for a moment. From the image above we see that our machine learning model seeks to predict the average value of a probability distribution. It could be any distribution but in our example is a normal distribution:


$$
\begin{equation}
  p(y) = \mathcal{N}(f(x), \sigma)
  \label{eq:ml_prob_prediction}
\end{equation}
$$

Where $$f(x)$$ is the model output and $$\sigma$$ is a parameter that accounts for the noise. Since the distribution familiy (normal in this case) and $$\sigma$$ parameter are fixed for a given problem, what we normally encounter is that the notation is relaxed and obviated, only focusing in $$f(x)$$, hiding the probability assumptions under the hood.

Thinking in probabilistic terms we can define the model goal as to find the most likely probability distribution for our dataset by finding the optimal $$f(x)$$.


$$
\begin{equation}
  \max p(Y) = \max_q q(Y) \qquad \qquad \text{Where } q \sim \mathcal{N}(f(x), \sigma)
  \label{eq:ml_max_likelihood}
\end{equation}
$$


## Deriving the optimisation problem over datasets

In the previous section we conclude that we want to maximise the probability for each data point per the last equation in the preivous section. Usually, in our problems we assume that all the samples are Independent and Identically Distributed (I.I.D.) so the probability of the entire dataset is equal to multiply each individual probabilities:

$$
\begin{equation}
  \max p(Y) = \max \prod_i p(y_i) =  \prod_i \max_q q(y_i) \qquad \qquad \text{Where } q \sim \mathcal{N}(f(x_i), \sigma)
  \label{eq:ml_max_likelihood_dataset}
\end{equation}
$$

For mathematical convenience we work with the log of the probability to convert the product to sumation:

$$
\begin{equation}
  \max \log p(Y) = \max \sum_i \log p(y_i) =  \max_q \sum_i \log q(y_i) \qquad \text{Where } q \sim \mathcal{N}(f(x_i), \sigma)
  \label{eq:ml_max_log_likelihood_dataset}
\end{equation}
$$

This is refered to as maximising the log-likelihood. Let's now expand the terms:


$$
\begin{equation}
  \begin{aligned}
  & \max \log p(Y) =  \max_q \log q(Y) \\
  & = \max_q \sum_i \log q(y_i) \\
  & = \max_f \sum_i \log \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{||y_i - f(x_i)||^2}{2 \sigma^2}} \\
  & = \max_f \sum_i \log \frac{1}{\sqrt{2\pi \sigma^2}} - \frac{1}{2 \sigma^2} ||y_i - f(x_i)||^2 \\
  & = \max_f \sum_i - \frac{1}{2 \sigma^2} ||y_i - f(x_i)||^2
 \end{aligned}
  \label{eq:ml_max_log_likelihood_dataset_to_ml}
\end{equation}
$$

The last step we got rid of $$\log \frac{1}{\sqrt{2\pi \sigma^2}}$$ since it does not depend on $$f(x)$$ so it is a constant that does not affect on the maximisation problem. Note the final result, does it sound familiar? Let's rewrite it a little bit further:

$$
\begin{equation}

\max_f \sum_i - \frac{1}{2 \sigma^2} ||y_i - f(x_i)||^2 = \min_f \sum_i \frac{1}{2 \sigma^2} ||y_i - f(x_i)||^2
\end{equation}
$$

If we set $$ \sigma = 1$$ we get the mean square error formula!

## The link

From the derivation in the last section we have reached an interesting result: fitting a model that estimates the mean of a normal distribution of unit variance maximising the probability of our datset is the same as applying mean square error! And that is where the probabilistic lense of the problem links with the error minimising approach.

This is something that is well known once you get your hands in the field but is usually overlooked in resources for students and new practioniers. The goal of this post is to give the insight, and derivation, on how this two views are the two faces of the same coin. Hopefully, it was able to achieve it!
